{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Ideia principal desse projeto é entender mais a fundo os seguintes conceitos, descrevelos aqui e implementa-los:\\n        - Transformers:\\n            - Como funcionam\\n            - Como serem utilizados como modelo base para uma tarefa NLP\\n        - Embedding:\\n            - Como funcionam\\n            - Como utilizar para embedar minha base de treinamento especifica (Kaggle)\\n            - Ao predizer, como retornar para o estado de linguagem humana\\n        - Fine Tuning:\\n            - Realizar a construção de um modelo especifico para alguma tarefa de NLP\\n                - Limpeza dos dados\\n                - Pre processamento:\\n                    - Remoção de stop words\\n                    - Stemmizar os tokens\\n                - Realizar uma pipeline:\\n                    - Conversão para embedding\\n                    - Construção do modelo para a task\\n        - Exibição para o usuário final:\\n            - Exibir graficos\\n                - Comparação entre metricas:\\n                    - F1 Score\\n                    - Recall\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Ideia principal desse projeto é entender mais a fundo os seguintes conceitos, descrevelos aqui e implementa-los:\n",
    "        - Transformers:\n",
    "            - Como funcionam\n",
    "            - Como serem utilizados como modelo base para uma tarefa NLP\n",
    "        - Embedding:\n",
    "            - Como funcionam\n",
    "            - Como utilizar para embedar minha base de treinamento especifica (Kaggle)\n",
    "            - Ao predizer, como retornar para o estado de linguagem humana\n",
    "        - Fine Tuning:\n",
    "            - Realizar a construção de um modelo especifico para alguma tarefa de NLP\n",
    "                - Limpeza dos dados\n",
    "                - Pre processamento:\n",
    "                    - Remoção de stop words\n",
    "                    - Stemmizar os tokens\n",
    "                - Realizar uma pipeline:\n",
    "                    - Conversão para embedding\n",
    "                    - Construção do modelo para a task\n",
    "        - Exibição para o usuário final:\n",
    "            - Exibir graficos\n",
    "                - Comparação entre metricas:\n",
    "                    - F1 Score\n",
    "                    - Recall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Para esse caso nós poderiamos utilizar muitas maneiras de vetorizar as palavras para realizar\\n    o fine tuning de nosso modelo, como por exemplo TFID Vectorizer, Word2Vec, entre outras.\\n    Mas por uma escolha mais precisa optamos por utilizar o BERT para embedar nossas palavras.\\n    E por que o BERT e não alguma dessas outras?\\n        - Em comparação entre BERT e Word2Vec, o BERT se sai melhor por conta de embedar usando o contexto\\n        da frase por completo, e não só gerar um embed estatico igual o Word2Vec, além de ter seu pré treino em\\n        uma base de dados muito maior ao do Word2Vec. E com isso BERT se torna um modelo melhor para tarefas de NLP.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Para esse caso nós poderiamos utilizar muitas maneiras de vetorizar as palavras para realizar\n",
    "    o fine tuning de nosso modelo, como por exemplo TFID Vectorizer, Word2Vec, entre outras.\n",
    "    Mas por uma escolha mais precisa optamos por utilizar o BERT para embedar nossas palavras.\n",
    "    E por que o BERT e não alguma dessas outras?\n",
    "        - Em comparação entre BERT e Word2Vec, o BERT se sai melhor por conta de embedar usando o contexto\n",
    "        da frase por completo, e não só gerar um embed estatico igual o Word2Vec, além de ter seu pré treino em\n",
    "        uma base de dados muito maior ao do Word2Vec. E com isso BERT se torna um modelo melhor para tarefas de NLP.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\janna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5842, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"] = np.where(df[\"Sentiment\"] == \"positive\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text):\n",
    "    \"\"\"\n",
    "    This function is used to tokenize each phrase, remove punctuation and stopwords,\n",
    "    after that stem each word and join each token to an string in lower case.\n",
    "    \"\"\"\n",
    "    text = [stemmer.stem(word) for word in text.split() if word not in punctuation and word not in STOPWORDS]\n",
    "    return \" \".join(text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentence\"] = df[\"Sentence\"].apply(pre_process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the geosolut technolog leverag benefon 's gps ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$esi lows, $1.50 $2.50 bk real possibl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for last quarter 2010 componenta 's net sale d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accord finnish-russian chamber commerc major c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the swedish buyout firm sold remain 22.4 perce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  the geosolut technolog leverag benefon 's gps ...          1\n",
       "1             $esi lows, $1.50 $2.50 bk real possibl          0\n",
       "2  for last quarter 2010 componenta 's net sale d...          1\n",
       "3  accord finnish-russian chamber commerc major c...          0\n",
       "4  the swedish buyout firm sold remain 22.4 perce...          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Sentence\"]\n",
    "y = df[[\"Sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4673,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = tokenizer( \n",
    "    text=X_train.tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4673, 512), dtype=int32, numpy=\n",
       "array([[  101,  2918,  2640, ...,     0,     0,     0],\n",
       "       [  101, 15253,  2080, ...,     0,     0,     0],\n",
       "       [  101, 15802, 12478, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  3389,  2094, ...,     0,     0,     0],\n",
       "       [  101,  6887, 27292, ...,     0,     0,     0],\n",
       "       [  101, 11090,  2368, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embed = tokenizer( \n",
    "    text=X_test.tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(encodings, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        labels\n",
    "    ))\n",
    "\n",
    "train_dataset = encode_dataset(X_train_embed, y_train)\n",
    "test_dataset = encode_dataset(X_test_embed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "585/585 [==============================] - 3972s 7s/step - loss: 0.6332 - accuracy: 0.6824\n",
      "Epoch 2/2\n",
      "585/585 [==============================] - 3845s 7s/step - loss: 0.6298 - accuracy: 0.6839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x1a89fdf64f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset.shuffle(100).batch(8), epochs=2, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Fontes utilizadas para pesquisa:\n",
    "        - https://www.geeksforgeeks.org/how-to-generate-word-embedding-using-bert/\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
